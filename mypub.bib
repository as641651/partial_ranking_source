
@inproceedings{sankaran2022test,
	title = {A {Test} for {FLOPs} as a {Discriminant} for {Linear} {Algebra} {Algorithms}},
	doi = {10.1109/SBAC-PAD55451.2022.00033},
	abstract = {Linear algebra expressions, which play a central role in countless scientific computations, are often computed via a sequence of calls to existing libraries of building blocks (such as those provided by BLAS and LAPACK). A sequence identifies a computing strategy, i.e., an algorithm, and normally for one linear algebra expression many alternative algorithms exist. Although mathematically equivalent, those algorithms might exhibit significant differences in terms of performance. Several high-level languages and tools for matrix computations such as Julia, Armadillo, Linnea, etc., make algorithmic choices by minimizing the number of Floating Point Operations (FLOPs). However, there can be several algorithms that share the same (or have nearly identical) number of FLOPs; in many cases, these algorithms exhibit execution times which are statistically equivalent and one could arbitrarily select one of them as the best algorithm. It is however not unlikely to find cases where the execution times are significantly different from one another (despite the FLOP count being almost the same). It is also possible that the algorithm that minimizes FLOPs is not the one that minimizes execution time. In this work, we develop a methodology to test the reliability of FLOPs as discriminant for linear algebra algorithms. Given a set of algorithms (for an instance of a linear algebra expression) as input, the methodology ranks them into performance classes; i.e., multiple algorithms are allowed to share the same rank. To this end, we measure the algorithms iteratively until the changes in the ranks converge to a value close to zero. FLOPs are a valid discriminant for an instance if all the algorithms with minimum FLOPs are assigned the best rank; otherwise, the instance is regarded as an anomaly, which can then be used in the investigation of the root cause of performance differences.},
	booktitle = {2022 {IEEE} 34th {International} {Symposium} on {Computer} {Architecture} and {High} {Performance} {Computing} ({SBAC}-{PAD})},
	author = {Sankaran, Aravind and Bientinesi, Paolo},
	month = nov,
	year = {2022},
	note = {ISSN: 2643-3001},
	keywords = {Algorithm ranking, High performance computing, Libraries, Linear algebra, Linear algebra algorithms, Mathematical software performance, Performance Analysis, Reliability, Software algorithms, Software performance, Time measurement},
	pages = {221--230},
	file = {IEEE Xplore Abstract Record:/Users/aravind/Zotero/storage/T7F2IGKV/9980912.html:text/html;IEEE Xplore Full Text PDF:/Users/aravind/Zotero/storage/G7PDYEWY/Sankaran and Bientinesi - 2022 - A Test for FLOPs as a Discriminant for Linear Alge.pdf:application/pdf},
}

@inproceedings{sankaran2021performance,
	title = {Performance {Comparison} for {Scientific} {Computations} on the {Edge} via {Relative} {Performance}},
	doi = {10.1109/IPDPSW52791.2021.00132},
	abstract = {In a typical Internet-of-Things setting that involves scientific applications, a target computation can be evaluated in many different ways depending on the split of computations among various devices. On the one hand, different implementations (or algorithms)—equivalent from a mathematical perspective—might exhibit significant difference in terms of performance. On the other hand, some of the implementations are likely to show similar performance characteristics. In this paper, we focus on analysing the performance of a given set of algorithms by clustering them into performance classes. To this end, we use a measurement-based approach to evaluate and score algorithms based on pair-wise comparisons; we refer to this approach as "Relative performance analysis". Each comparison yields one of three outcomes: one algorithm can be "better", "worse", or "equivalent" to another; those algorithms evaluating to have "equivalent" performance are merged into the same performance class. We show that our clustering methodology facilitates algorithm selection with respect to more than one metric; for instance, from the subset of equivalently fast algorithms, one could then select an algorithm that consumes the least energy on a certain device.},
	booktitle = {2021 {IEEE} {International} {Parallel} and {Distributed} {Processing} {Symposium} {Workshops} ({IPDPSW})},
	author = {Sankaran, Aravind and Bientinesi, Paolo},
	month = jun,
	year = {2021},
	keywords = {algorithm ranking, clustering, Clustering algorithms, Conferences, distributed computing, Distributed processing, edge computing, performance analysis, Performance analysis, Performance evaluation, scientific computing},
	pages = {887--895},
	file = {IEEE Xplore Abstract Record:/Users/aravind/Zotero/storage/5B62TLQD/9460671.html:text/html;IEEE Xplore Full Text PDF:/Users/aravind/Zotero/storage/TGAXWC9A/Sankaran and Bientinesi - 2021 - Performance Comparison for Scientific Computations.pdf:application/pdf},
}

@inproceedings{sankaran2021performance-1,
	title = {Performance {Comparison} for {Scientific} {Computations} on the {Edge} via {Relative} {Performance}},
	doi = {10.1109/IPDPSW52791.2021.00132},
	abstract = {In a typical Internet-of-Things setting that involves scientific applications, a target computation can be evaluated in many different ways depending on the split of computations among various devices. On the one hand, different implementations (or algorithms)—equivalent from a mathematical perspective—might exhibit significant difference in terms of performance. On the other hand, some of the implementations are likely to show similar performance characteristics. In this paper, we focus on analysing the performance of a given set of algorithms by clustering them into performance classes. To this end, we use a measurement-based approach to evaluate and score algorithms based on pair-wise comparisons; we refer to this approach as "Relative performance analysis". Each comparison yields one of three outcomes: one algorithm can be "better", "worse", or "equivalent" to another; those algorithms evaluating to have "equivalent" performance are merged into the same performance class. We show that our clustering methodology facilitates algorithm selection with respect to more than one metric; for instance, from the subset of equivalently fast algorithms, one could then select an algorithm that consumes the least energy on a certain device.},
	booktitle = {2021 {IEEE} {International} {Parallel} and {Distributed} {Processing} {Symposium} {Workshops} ({IPDPSW})},
	author = {Sankaran, Aravind and Bientinesi, Paolo},
	month = jun,
	year = {2021},
	keywords = {algorithm ranking, clustering, Clustering algorithms, Conferences, distributed computing, Distributed processing, edge computing, performance analysis, Performance analysis, Performance evaluation, scientific computing},
	pages = {887--895},
	file = {IEEE Xplore Abstract Record:/Users/aravind/Zotero/storage/QGUAZHYM/9460671.html:text/html;IEEE Xplore Full Text PDF:/Users/aravind/Zotero/storage/NPEZLFHA/Sankaran and Bientinesi - 2021 - Performance Comparison for Scientific Computations.pdf:application/pdf},
}
