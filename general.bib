
@article{nikitenko2021influence,
	title = {Influence of {Noisy} {Environments} on {Behavior} of {HPC} {Applications}},
	volume = {42},
	issn = {1818-9962},
	url = {https://doi.org/10.1134/S1995080221070192},
	doi = {10.1134/S1995080221070192},
	abstract = {Many contemporary HPC systems expose their jobs to substantial amounts of interference, leading to significant run-to-run variation. For example, application runtimes on Theta, a Cray XC40 system at Argonne National Laboratory, vary by up to 70\$\${\textbackslash}\%\$\$, caused by a mix of node-level and system-level effects, including network and file-system congestion in the presence of concurrently running jobs. This makes performance measurements generally irreproducible, heavily complicating performance analysis and modeling. On noisy systems, performance analysts usually have to repeat performance measurements several times and then apply statistics to capture trends. First, this is expensive and, second, extracting trends from a limited series of experiments is far from trivial, as the noise can follow quite irregular patterns. Attempts to learn from performance data how a program would perform under different execution configurations experience serious perturbation, resulting in models that reflect noise rather than intrinsic application behavior. On the other hand, although noise heavily influences execution time and energy consumption, it does not change the computational effort a program performs. Effort metrics that count how many operations a machine executes on behalf of a program, such as floating-point operations, the exchange of MPI messages, or file reads and writes, remain largely unaffected and—rare non-determinism set aside—reproducible. This paper addresses initial stage of an ExtraNoise project, which is aimed at revealing and tackling key questions of system noise influence on HPC applications.},
	language = {en},
	number = {7},
	urldate = {2023-06-22},
	journal = {Lobachevskii Journal of Mathematics},
	author = {Nikitenko, D. A. and Wolf, F. and Mohr, B. and Hoefler, T. and Stefanov, K. S. and Voevodin, Vad. V. and Antonov, A. S. and Calotoiu, A.},
	month = jul,
	year = {2021},
	keywords = {noise},
	pages = {1560--1570},
	file = {Full Text PDF:/Users/aravind/Zotero/storage/XH36M8VR/Nikitenko et al. - 2021 - Influence of Noisy Environments on Behavior of HPC.pdf:application/pdf},
}

@inproceedings{peise2015study,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {A {Study} on the {Influence} of {Caching}: {Sequences} of {Dense} {Linear} {Algebra} {Kernels}},
	isbn = {978-3-319-17353-5},
	shorttitle = {A {Study} on the {Influence} of {Caching}},
	doi = {10.1007/978-3-319-17353-5_21},
	abstract = {It is universally known that caching is critical to attain high-performance implementations: In many situations, data locality (in space and time) plays a bigger role than optimizing the (number of) arithmetic floating point operations. In this paper, we show evidence that at least for linear algebra algorithms, caching is also a crucial factor for accurate performance modeling and performance prediction.},
	language = {en},
	booktitle = {High {Performance} {Computing} for {Computational} {Science} -- {VECPAR} 2014},
	publisher = {Springer International Publishing},
	author = {Peise, Elmar and Bientinesi, Paolo},
	editor = {Daydé, Michel and Marques, Osni and Nakajima, Kengo},
	year = {2015},
	keywords = {cache, noise},
	pages = {245--258},
	file = {Full Text PDF:/Users/aravind/Zotero/storage/VYTMQVKU/Peise and Bientinesi - 2015 - A Study on the Influence of Caching Sequences of .pdf:application/pdf},
}

@inproceedings{acun2016variation,
	address = {New York, NY, USA},
	series = {{ICS} '16},
	title = {Variation {Among} {Processors} {Under} {Turbo} {Boost} in {HPC} {Systems}},
	isbn = {978-1-4503-4361-9},
	url = {https://dl.acm.org/doi/10.1145/2925426.2926289},
	doi = {10.1145/2925426.2926289},
	abstract = {The design and manufacture of present-day CPUs causes inherent variation in supercomputer architectures such as variation in power and temperature of the chips. The variation also manifests itself as frequency differences among processors under Turbo Boost dynamic overclocking. This variation can lead to unpredictable and suboptimal performance in tightly coupled HPC applications. In this study, we use compute-intensive kernels and applications to analyze the variation among processors in four top supercomputers: Edison, Cab, Stampede, and Blue Waters. We observe that there is an execution time difference of up to 16\% among processors on the Turbo Boost-enabled supercomputers: Edison, Cab, Stampede. There is less than 1\% variation on Blue Waters, which does not have a dynamic overclocking feature. We analyze measurements from temperature and power instrumentation and find that intrinsic differences in the chips' power efficiency is the culprit behind the frequency variation. Moreover, we analyze potential solutions such as disabling Turbo Boost, leaving idle cores and replacing slow chips to mitigate the variation. We also propose a speed-aware dynamic task redistribution (load balancing) algorithm to reduce the negative effects of performance variation. Our speed-aware load balancing algorithm improves the performance up to 18\% compared to no load balancing performance and 6\% better than the non-speed aware counterpart.},
	urldate = {2023-06-22},
	booktitle = {Proceedings of the 2016 {International} {Conference} on {Supercomputing}},
	publisher = {Association for Computing Machinery},
	author = {Acun, Bilge and Miller, Phil and Kale, Laxmikant V.},
	month = jun,
	year = {2016},
	keywords = {noise},
	pages = {1--12},
	file = {Full Text PDF:/Users/aravind/Zotero/storage/WRB998MP/Acun et al. - 2016 - Variation Among Processors Under Turbo Boost in HP.pdf:application/pdf},
}

@inproceedings{charles2009evaluation,
	title = {Evaluation of the {Intel}® {Core}™ i7 {Turbo} {Boost} feature},
	doi = {10.1109/IISWC.2009.5306782},
	abstract = {The Intel® Core™ i7 processor code named Nehalem has a novel feature called Turbo Boost which dynamically varies the frequencies of the processor's cores. The frequency of a core is determined by core temperature, the number of active cores, the estimated power and the estimated current consumption. We perform an extensive analysis of the Turbo Boost technology to characterize its behavior in varying workload conditions. In particular, we analyze how the activation of Turbo Boost is affected by inherent properties of applications (i.e., their rate of memory accesses) and by the overall load imposed on the processor. Furthermore, we analyze the capability of Turbo Boost to mitigate Amdahl's law by accelerating sequential phases of parallel applications. Finally, we estimate the impact of the Turbo Boost technology on the overall energy consumption. We found that Turbo Boost can provide (on average) up to a 6\% reduction in execution time but can result in an increase in energy consumption up to 16\%. Our results also indicate that Turbo Boost sets the processor to operate at maximum frequency (where it has the potential to provide the maximum gain in performance) when the mapping of threads to hardware contexts is sub-optimal.},
	booktitle = {2009 {IEEE} {International} {Symposium} on {Workload} {Characterization} ({IISWC})},
	author = {Charles, James and Jassi, Preet and Ananth, Narayan S and Sadat, Abbas and Fedorova, Alexandra},
	month = oct,
	year = {2009},
	keywords = {Acceleration, Application software, Energy consumption, Frequency estimation, Leakage current, Multicore processing, noise, Performance analysis, Temperature, Voltage, Yarn},
	pages = {188--197},
	file = {IEEE Xplore Abstract Record:/Users/aravind/Zotero/storage/R37IW8MH/5306782.html:text/html;IEEE Xplore Full Text PDF:/Users/aravind/Zotero/storage/Y8PGF5IK/Charles et al. - 2009 - Evaluation of the Intel® Core™ i7 Turbo Boost feat.pdf:application/pdf},
}

@article{chen2015statistical,
	title = {Statistical {Performance} {Comparisons} of {Computers}},
	volume = {64},
	issn = {1557-9956},
	doi = {10.1109/TC.2014.2315614},
	abstract = {As a fundamental task in computer architecture research, performance comparison has been continuously hampered by the variability of computer performance. In traditional performance comparisons, the impact of performance variability is usually ignored (i.e., the means of performance observations are compared regardless of the variability), or in the few cases directly addressed with t -statistics without checking the number and normality of performance observations. In this paper, we formulate a performance comparison as a statistical task, and empirically illustrate why and how common practices can lead to incorrect comparisons. We propose a non-parametric hierarchical performance testing (HPT) framework for performance comparison, which is significantly more practical than standard t -statistics because it does not require to collect a large number of performance observations in order to achieve a normal distribution of sample mean. In particular, the proposed HPT can facilitate quantitative performance comparison, in which the performance speedup of one computer over another is statistically evaluated. Compared with the HPT, a common practice which uses geometric mean performance scores to estimate the performance speedup has errors of 8.0 to 56.3 percent on SPEC CPU2006 or SPEC MPI2007, which demonstrates the necessity of using appropriate statistical techniques. This HPT framework has been implemented as an open-source software, and integrated in the PARSEC 3.0 benchmark suite.},
	number = {5},
	journal = {IEEE Transactions on Computers},
	author = {Chen, Tianshi and Guo, Qi and Temam, Olivier and Wu, Yue and Bao, Yungang and Xu, Zhiwei and Chen, Yunji},
	month = may,
	year = {2015},
	note = {Conference Name: IEEE Transactions on Computers},
	keywords = {Benchmark testing, Computer architecture, Computer performance, Computers, hierarchical performance testing, noise, Performance comparison, performance distribution, Probability distribution, Reliability, t -statistics},
	pages = {1442--1455},
	file = {IEEE Xplore Abstract Record:/Users/aravind/Zotero/storage/J2RCTAIH/6783811.html:text/html;IEEE Xplore Full Text PDF:/Users/aravind/Zotero/storage/CCNW33KC/Chen et al. - 2015 - Statistical Performance Comparisons of Computers.pdf:application/pdf},
}

@inproceedings{hoefler2015scientific,
	address = {New York, NY, USA},
	series = {{SC} '15},
	title = {Scientific benchmarking of parallel computing systems: twelve ways to tell the masses when reporting performance results},
	isbn = {978-1-4503-3723-6},
	shorttitle = {Scientific benchmarking of parallel computing systems},
	url = {https://dl.acm.org/doi/10.1145/2807591.2807644},
	doi = {10.1145/2807591.2807644},
	abstract = {Measuring and reporting performance of parallel computers constitutes the basis for scientific advancement of high-performance computing (HPC). Most scientific reports show performance improvements of new techniques and are thus obliged to ensure reproducibility or at least interpretability. Our investigation of a stratified sample of 120 papers across three top conferences in the field shows that the state of the practice is lacking. For example, it is often unclear if reported improvements are deterministic or observed by chance. In addition to distilling best practices from existing work, we propose statistically sound analysis and reporting techniques and simple guidelines for experimental design in parallel computing and codify them in a portable benchmarking library. We aim to improve the standards of reporting research results and initiate a discussion in the HPC field. A wide adoption of our minimal set of rules will lead to better interpretability of performance results and improve the scientific culture in HPC.},
	urldate = {2023-06-22},
	booktitle = {Proceedings of the {International} {Conference} for {High} {Performance} {Computing}, {Networking}, {Storage} and {Analysis}},
	publisher = {Association for Computing Machinery},
	author = {Hoefler, Torsten and Belli, Roberto},
	month = nov,
	year = {2015},
	keywords = {benchmarking, data analysis, noise, parallel computing, statistics},
	pages = {1--12},
	file = {Full Text PDF:/Users/aravind/Zotero/storage/I682BTJT/Hoefler and Belli - 2015 - Scientific benchmarking of parallel computing syst.pdf:application/pdf},
}

@inproceedings{marker2014understanding,
	address = {Vasteras Sweden},
	title = {Understanding performance stairs: elucidating heuristics},
	isbn = {978-1-4503-3013-8},
	shorttitle = {Understanding performance stairs},
	url = {https://dl.acm.org/doi/10.1145/2642937.2642975},
	doi = {10.1145/2642937.2642975},
	language = {en},
	urldate = {2023-06-30},
	booktitle = {Proceedings of the 29th {ACM}/{IEEE} {International} {Conference} on {Automated} {Software} {Engineering}},
	publisher = {ACM},
	author = {Marker, Bryan and Batory, Don and Van De Geijn, Robert},
	month = sep,
	year = {2014},
	pages = {301--312},
}

@article{barthels2021linnea,
	title = {Linnea: {Automatic} {Generation} of {Efficient} {Linear} {Algebra} {Programs}},
	volume = {47},
	issn = {0098-3500},
	shorttitle = {Linnea},
	url = {https://dl.acm.org/doi/10.1145/3446632},
	doi = {10.1145/3446632},
	abstract = {The translation of linear algebra computations into efficient sequences of library calls is a non-trivial task that requires expertise in both linear algebra and high-performance computing. Almost all high-level languages and libraries for matrix computations (e.g., Matlab, Eigen) internally use optimized kernels such as those provided by BLAS and LAPACK; however, their translation algorithms are often too simplistic and thus lead to a suboptimal use of said kernels, resulting in significant performance losses. To combine the productivity offered by high-level languages, and the performance of low-level kernels, we are developing Linnea, a code generator for linear algebra problems. As input, Linnea takes a high-level description of a linear algebra problem; as output, it returns an efficient sequence of calls to high-performance kernels. Linnea uses a custom best-first search algorithm to find a first solution in less than a second, and increasingly better solutions when given more time. In 125 test problems, the code generated by Linnea almost always outperforms Matlab, Julia, Eigen, and Armadillo, with speedups up to and exceeding 10×.},
	number = {3},
	urldate = {2023-06-30},
	journal = {ACM Transactions on Mathematical Software},
	author = {Barthels, Henrik and Psarras, Christos and Bientinesi, Paolo},
	month = jun,
	year = {2021},
	keywords = {code generation, Linear algebra},
	pages = {22:1--22:26},
	file = {Full Text PDF:/Users/aravind/Zotero/storage/YKU7N4T8/Barthels et al. - 2021 - Linnea Automatic Generation of Efficient Linear A.pdf:application/pdf},
}

@article{psarras2022linear,
	title = {The {Linear} {Algebra} {Mapping} {Problem}. {Current} {State} of {Linear} {Algebra} {Languages} and {Libraries}},
	volume = {48},
	issn = {0098-3500, 1557-7295},
	url = {https://dl.acm.org/doi/10.1145/3549935},
	doi = {10.1145/3549935},
	abstract = {We observe a disconnect between developers and end-users of linear algebra libraries. On the one hand, developers invest significant effort in creating sophisticated numerical kernels. On the other hand, end-users are progressively less likely to go through the time consuming process of directly using said kernels; instead, languages and libraries, which offer a higher level of abstraction, are becoming increasingly popular. These languages offer mechanisms that internally map the input program to lower level kernels. Unfortunately, our experience suggests that, in terms of performance, this translation is typically suboptimal.
            
              In this paper, we define the problem of mapping a linear algebra expression to a set of available building blocks as the
              “Linear Algebra Mapping Problem” (LAMP)
              ; we discuss its NP-complete nature, and investigate how effectively a benchmark of test problems is solved by popular high-level programming languages and libraries. Specifically, we consider Matlab, Octave, Julia, R, Armadillo (C++), Eigen (C++), and NumPy (Python); the benchmark is meant to test both compiler optimizations, as well as linear algebra specific optimizations, such as the optimal parenthesization of matrix products. The aim of this study is to facilitate the development of languages and libraries that support linear algebra computations.},
	language = {en},
	number = {3},
	urldate = {2023-06-30},
	journal = {ACM Transactions on Mathematical Software},
	author = {Psarras, Christos and Barthels, Henrik and Bientinesi, Paolo},
	month = sep,
	year = {2022},
	pages = {1--30},
	file = {Submitted Version:/Users/aravind/Zotero/storage/6U2VAKKC/Psarras et al. - 2022 - The Linear Algebra Mapping Problem. Current State .pdf:application/pdf},
}

@article{bezanson2017julia,
	title = {Julia: {A} {Fresh} {Approach} to {Numerical} {Computing}},
	volume = {59},
	issn = {0036-1445, 1095-7200},
	shorttitle = {Julia},
	url = {https://epubs.siam.org/doi/10.1137/141000671},
	doi = {10.1137/141000671},
	language = {en},
	number = {1},
	urldate = {2023-06-30},
	journal = {SIAM Review},
	author = {Bezanson, Jeff and Edelman, Alan and Karpinski, Stefan and Shah, Viral B.},
	month = jan,
	year = {2017},
	pages = {65--98},
	file = {Full Text:/Users/aravind/Zotero/storage/2CY5TRIJ/Bezanson et al. - 2017 - Julia A Fresh Approach to Numerical Computing.pdf:application/pdf},
}

@article{noauthor2002updated,
	title = {An updated set of basic linear algebra subprograms ({BLAS})},
	volume = {28},
	issn = {0098-3500, 1557-7295},
	url = {https://dl.acm.org/doi/10.1145/567806.567807},
	doi = {10.1145/567806.567807},
	language = {en},
	number = {2},
	urldate = {2023-06-30},
	journal = {ACM Transactions on Mathematical Software},
	month = jun,
	year = {2002},
	pages = {135--151},
}

@book{anderson1990lapack,
	title = {{LAPACK}: {A} {Portable} {Linear} {Algebra} {Library} for {High}-{Performance} {Computers}},
	shorttitle = {{LAPACK}},
	abstract = {The goal of the LAPACK project is to design and implement a portable linear algebra library for efficient use on a variety of high-performance computers. The library is based on the widely used LINPACK and EISPACK packages for solving linear equations, eigenvalue problems, and linear least-squares problems, but extends their functionality in a number of ways. The major methodology for making the algorithms run faster is to restructure them to perform block matrix operations (e.g., matrix-matrix multiplication) in their inner loops. These block operations may be optimized to exploit the memory hierarchy of a specific architecture. The LAPACK project is also working on new algorithms that yield higher relative accuracy for a variety of linear algebra problems.},
	author = {Anderson, Ed and Bai, Zhaojun and Dongarra, Jack and Greenbaum, A. and McKenney, A. and Croz, Jeremy and Hammarling, Sven and Demmel, James and Bischof, Christian and Sorensen, Danny},
	month = jan,
	year = {1990},
	note = {Journal Abbreviation: [No source information available]
Pages: 11
Publication Title: [No source information available]},
	file = {Full Text PDF:/Users/aravind/Zotero/storage/QF976JWU/Anderson et al. - 1990 - LAPACK A Portable Linear Algebra Library for High.pdf:application/pdf},
}
