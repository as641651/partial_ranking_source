
@article{kumari2017machine,
	title = {Machine {Learning}: {A} {Review} on {Binary} {Classification}},
	volume = {160},
	issn = {09758887},
	shorttitle = {Machine {Learning}},
	url = {http://www.ijcaonline.org/archives/volume160/number7/kumari-2017-ijca-913083.pdf},
	doi = {10.5120/ijca2017913083},
	number = {7},
	urldate = {2023-06-30},
	journal = {International Journal of Computer Applications},
	author = {Kumari, Roshan and Kr., Saurabh},
	month = feb,
	year = {2017},
	pages = {11--15},
	file = {Full Text:C\:\\Users\\aravind\\Zotero\\storage\\ZPTW7NVC\\Kumari and Kr. - 2017 - Machine Learning A Review on Binary Classificatio.pdf:application/pdf},
}

@incollection{van_der_aalst2019structuring,
	address = {Cham},
	title = {Structuring {Behavior} or {Not}, {That} is the {Question}},
	isbn = {978-3-030-06234-7},
	url = {https://doi.org/10.1007/978-3-030-06234-7_21},
	abstract = {Process models aim to structure behavior for a variety of reasons: discussion, analysis, improvement, implementation, and automation. Traditionally, process models were obtained through modeling and structure could be enforced, e.g., by streamlining or simplifying processes. However, process discovery techniques that start from the actual behavior shed new light on this. These techniques return process models that are either formal (precisely describing the possible behaviors) or informal (merely a “picture” not allowing for any form of formal reasoning). Both types of model aim to structure reality. However, reality is often very different and much more variable than expected by stakeholders. Process mining often reveals an “inconvenient truth” which provides the valuable insights needed to improve a wide variety of processes. This contribution, devoted to Jörg Becker’s 60th birthday, reflects on the notion of “structure” in a world where event data are omnipresent.},
	language = {en},
	urldate = {2023-06-30},
	booktitle = {The {Art} of {Structuring}: {Bridging} the {Gap} {Between} {Information} {Systems} {Research} and {Practice}},
	publisher = {Springer International Publishing},
	author = {van der Aalst, Wil},
	editor = {Bergener, Katrin and Räckers, Michael and Stein, Armin},
	year = {2019},
	doi = {10.1007/978-3-030-06234-7_21},
	keywords = {Business process management, Process discovery, Process mining, Vagueness in models},
	pages = {221--226},
	file = {Full Text PDF:C\:\\Users\\aravind\\Zotero\\storage\\YBES3NXY\\van der Aalst - 2019 - Structuring Behavior or Not, That is the Question.pdf:application/pdf},
}

@misc{businesswire2019celonis,
	title = {Celonis {Gains} {New} {Customers} {Across} {Industries} and {Geographies} with {Its} {Recently} {Released} {Intelligent} {Business} {Cloud}},
	url = {https://www.businesswire.com/news/home/20190114005089/en/Celonis-Gains-New-Customers-Across-Industries-and-Geographies-with-Its-Recently-Released-Intelligent-Business-Cloud},
	abstract = {Celonis, the leader in business transformation software, today announced new global customers leveraging its Intelligent Business Cloud to capture the},
	language = {en},
	urldate = {2023-06-30},
	author = {businesswire, BW},
	month = jan,
	year = {2019},
	file = {Snapshot:C\:\\Users\\aravind\\Zotero\\storage\\KBXWY7JY\\Celonis-Gains-New-Customers-Across-Industries-and-Geographies-with-Its-Recently-Released-Intell.html:text/html},
}

@incollection{van_der_aalst2016process,
	address = {Berlin, Heidelberg},
	title = {Process {Mining} {Software}},
	isbn = {978-3-662-49851-4},
	url = {https://doi.org/10.1007/978-3-662-49851-4_11},
	abstract = {The successful application of process mining relies on good tool support. Traditional Business Intelligence (BI) tools are data-centric and focus on rather simplistic forms of analysis. Mainstream data mining and machine learning tools provide more sophisticated forms of analysis, but are also not tailored towards the analysis and improvement of processes. Fortunately, there are dedicated process mining tools able to transform event data into actionable process-related insights. For example, ProM is an open-source process mining tool supporting all of the techniques mentioned in this book. Process discovery, conformance checking, social network analysis, organizational mining, clustering, decision mining, prediction, and recommendation are all supported by ProM plug-ins. However, the usability of the hundreds of available plug-ins varies and the complexity of the tool may be overwhelming for end-users. In recent years, several vendors released dedicated process mining tools (e.g., Celonis, Disco, EDS, Fujitsu, Minit, myInvenio, Perceptive, PPM, QPR, Rialto, and SNP). These tools typically provide less functionality than ProM, but are easier to use while focusing on data extraction, performance analysis and scalability. This chapter provides an overview of available tools and trends.},
	language = {en},
	urldate = {2023-06-30},
	booktitle = {Process {Mining}: {Data} {Science} in {Action}},
	publisher = {Springer},
	author = {van der Aalst, Wil},
	editor = {van der Aalst, Wil},
	year = {2016},
	doi = {10.1007/978-3-662-49851-4_11},
	keywords = {Business Intelligence, Data Mining Tool, Event Data, Mining Tool, Process Mining},
	pages = {325--352},
	file = {Full Text PDF:C\:\\Users\\aravind\\Zotero\\storage\\8IQ5LMRT\\van der Aalst - 2016 - Process Mining Software.pdf:application/pdf},
}

@article{taymouri2021business,
	title = {Business process variant analysis: {Survey} and classification},
	volume = {211},
	issn = {0950-7051},
	shorttitle = {Business process variant analysis},
	url = {https://www.sciencedirect.com/science/article/pii/S0950705120306869},
	doi = {10.1016/j.knosys.2020.106557},
	abstract = {It is common for business processes to exhibit a high degree of internal heterogeneity, in the sense that the executions of the process differ widely from each other due to contextual factors, human factors, or deliberate business decisions. For example, a quote-to-cash process in a multinational company is typically executed differently across different countries or even across different regions in the same country. Similarly, an insurance claims handling process might be executed differently across different claims handling centers or across multiple teams within the same claims handling center. A subset of executions of a business process that can be distinguished from others based on a given predicate (e.g. the executions of a process in a given country) is called a process variant. Understanding differences between process variants helps analysts and managers to make informed decisions as to how to standardize or otherwise improve a business process, for example by helping them find out what makes it that a given variant exhibits a higher performance than another one. Process variant analysis is a family of techniques to analyze event logs produced during the execution of a process, in order to identify and explain the differences between two or more process variants. A wide range of methods for process variant analysis have been proposed in the past decade. However, due to the interdisciplinary nature of this field, the proposed methods and the types of differences they can identify vary widely, and there is a lack of a unifying view of the field. To close this gap, this article presents a systematic literature review of methods for process variant analysis. The identified studies are classified according to their inputs, outputs, analysis purpose, underpinning algorithms, and extra-functional characteristics. The paper closes with a broad classification of approaches into three categories based on the paradigm they employ to compare multiple process variants.},
	language = {en},
	urldate = {2023-06-30},
	journal = {Knowledge-Based Systems},
	author = {Taymouri, Farbod and Rosa, Marcello La and Dumas, Marlon and Maggi, Fabrizio Maria},
	month = jan,
	year = {2021},
	keywords = {Business process management, Process mining, Machine learning},
	pages = {106557},
	file = {ScienceDirect Full Text PDF:C\:\\Users\\aravind\\Zotero\\storage\\7G3WBHCA\\Taymouri et al. - 2021 - Business process variant analysis Survey and clas.pdf:application/pdf;ScienceDirect Snapshot:C\:\\Users\\aravind\\Zotero\\storage\\4IFPEZNH\\S0950705120306869.html:text/html},
}

@book{szekli1995stochastic,
	address = {New York, NY},
	series = {Lecture {Notes} in {Statistics}},
	title = {Stochastic {Ordering} and {Dependence} in {Applied} {Probability}},
	volume = {97},
	isbn = {978-0-387-94450-0 978-1-4612-2528-7},
	url = {http://link.springer.com/10.1007/978-1-4612-2528-7},
	urldate = {2023-06-30},
	publisher = {Springer},
	author = {Szekli, R.},
	editor = {Fienberg, S. and Gani, J. and Krickeberg, K. and Olkin, I. and Wermuth, N.},
	year = {1995},
	doi = {10.1007/978-1-4612-2528-7},
	keywords = {Jackson network, Markov process, Markov renewal process, Martingal, Martingale, point process, probability, probability space, Random variable},
	file = {Full Text:C\:\\Users\\aravind\\Zotero\\storage\\5TND4FXA\\Szekli - 1995 - Stochastic Ordering and Dependence in Applied Prob.pdf:application/pdf},
}

@article{li2010ranking,
	title = {Ranking continuous probabilistic datasets},
	volume = {3},
	issn = {2150-8097},
	url = {https://dl.acm.org/doi/10.14778/1920841.1920923},
	doi = {10.14778/1920841.1920923},
	abstract = {Ranking is a fundamental operation in data analysis and decision support, and plays an even more crucial role if the dataset being explored exhibits uncertainty. This has led to much work in understanding how to rank uncertain datasets in recent years. In this paper, we address the problem of ranking when the tuple scores are uncertain, and the uncertainty is captured using continuous probability distributions (e.g. Gaussian distributions). We present a comprehensive solution to compute the values of a parameterized ranking function (PRF) [18] for arbitrary continuous probability distributions (and thus rank the uncertain dataset); PRF can be used to simulate or approximate many other ranking functions proposed in prior work. We develop exact polynomial time algorithms for some continuous probability distribution classes, and efficient approximation schemes with provable guarantees for arbitrary probability distributions. Our algorithms can also be used for exact or approximate evaluation of k-nearest neighbor queries over uncertain objects, whose positions are modeled using continuous probability distributions. Our experimental evaluation over several datasets illustrates the effectiveness of our approach at efficiently ranking uncertain datasets with continuous attribute uncertainty.},
	number = {1-2},
	urldate = {2023-06-30},
	journal = {Proceedings of the VLDB Endowment},
	author = {Li, Jian and Deshpande, Amol},
	month = sep,
	year = {2010},
	pages = {638--649},
	file = {Full Text PDF:C\:\\Users\\aravind\\Zotero\\storage\\I5FYPVWJ\\Li and Deshpande - 2010 - Ranking continuous probabilistic datasets.pdf:application/pdf},
}

@article{ilyas2008survey,
	title = {A survey of top-k query processing techniques in relational database systems},
	volume = {40},
	issn = {0360-0300},
	url = {https://dl.acm.org/doi/10.1145/1391729.1391730},
	doi = {10.1145/1391729.1391730},
	abstract = {Efficient processing of top-k queries is a crucial requirement in many interactive environments that involve massive amounts of data. In particular, efficient top-k processing in domains such as the Web, multimedia search, and distributed systems has shown a great impact on performance. In this survey, we describe and classify top-k processing techniques in relational databases. We discuss different design dimensions in the current techniques including query models, data access methods, implementation levels, data and query certainty, and supported scoring functions. We show the implications of each dimension on the design of the underlying techniques. We also discuss top-k queries in XML domain, and show their connections to relational approaches.},
	number = {4},
	urldate = {2023-06-30},
	journal = {ACM Computing Surveys},
	author = {Ilyas, Ihab F. and Beskales, George and Soliman, Mohamed A.},
	month = oct,
	year = {2008},
	keywords = {rank aggregation, rank-aware processing, Top-k, voting},
	pages = {11:1--11:58},
	file = {Full Text PDF:C\:\\Users\\aravind\\Zotero\\storage\\NHIRKMW5\\Ilyas et al. - 2008 - A survey of top-k query processing techniques in r.pdf:application/pdf},
}

@article{astrachan2003bubble,
	title = {Bubble sort: an archaeological algorithmic analysis},
	volume = {35},
	issn = {0097-8418},
	shorttitle = {Bubble sort},
	url = {https://dl.acm.org/doi/10.1145/792548.611918},
	doi = {10.1145/792548.611918},
	abstract = {Text books, including books for general audiences, invariably mention bubble sort in discussions of elementary sorting algorithms. We trace the history of bubble sort, its popularity, and its endurance in the face of pedagogical assertions that code and algorithmic examples used in early courses should be of high quality and adhere to established best practices. This paper is more an historical analysis than a philosophical treatise for the exclusion of bubble sort from books and courses. However, sentiments for exclusion are supported by Knuth [17], "In short, the bubble sort seems to have nothing to recommend it, except a catchy name and the fact that it leads to some interesting theoretical problems." Although bubble sort may not be a best practice sort, perhaps the weight of history is more than enough to compensate and provide for its longevity.},
	number = {1},
	urldate = {2023-06-22},
	journal = {ACM SIGCSE Bulletin},
	author = {Astrachan, Owen},
	month = jan,
	year = {2003},
	keywords = {analysis, bubble sort, performance},
	pages = {1--5},
	file = {Full Text PDF:C\:\\Users\\aravind\\Zotero\\storage\\UYIT6TJJ\\Astrachan - 2003 - Bubble sort an archaeological algorithmic analysi.pdf:application/pdf},
}

@article{pavan2004new,
	series = {Papers presented at the 5th {COLLOQUIUM} {CHEMIOMETRICUM} {MEDITERRANEUM}},
	title = {New indices for analysing partial ranking diagrams},
	volume = {515},
	issn = {0003-2670},
	url = {https://www.sciencedirect.com/science/article/pii/S0003267003014879},
	doi = {10.1016/j.aca.2003.11.019},
	abstract = {Interest is growing in decision making strategies and several techniques are now available. The assessment of priorities is a typical premise before final decisions are taken. Total and partial order ranking (POR) strategies, which from a mathematical point of view are based on elementary methods of discrete mathematics, appear as an attractive and simple tool to asses priorities. Despite the well-known total ranking strategies, which are scalar methods combining the different criteria values into a global index which always ranks elements in an ordered sequence, the partial order ranking is a vectorial approach which recognises that not all the elements can be directly compared with all the others. In fact when many criteria are considered, contradictions in the ranking are bound to exist and the higher the number of criteria, the higher the probability that contradictions in the ranking occur. The Hasse diagram technique (HDT) is a very useful tool to perform partial order ranking. The results of the partial order ranking are visualised in a diagram, called Hasse diagram. Incomparable elements are located at the same geometrical height and as high as possible in the diagram, thus incomparable elements are arranged in levels. The quality of a ranking procedure has to be evaluated by a deep analysis and by several indices, i.e. scalar functions that describe features of an ordered set and allow comparison among different rankings. For this purpose, new indices for ranking analysis are proposed here, compared with the ones found in literature and tested on theoretical examples and on real data.},
	language = {en},
	number = {1},
	urldate = {2023-06-22},
	journal = {Analytica Chimica Acta},
	author = {Pavan, M and Todeschini, R},
	month = jul,
	year = {2004},
	keywords = {Hasse diagrams, Multicriteria decision making, Partial order ranking, Priority setting, Ranking indices},
	pages = {167--181},
	file = {ScienceDirect Full Text PDF:C\:\\Users\\aravind\\Zotero\\storage\\YPUDFHQF\\Pavan and Todeschini - 2004 - New indices for analysing partial ranking diagrams.pdf:application/pdf;ScienceDirect Snapshot:C\:\\Users\\aravind\\Zotero\\storage\\AU96J2AC\\S0003267003014879.html:text/html},
}

@article{ye2016method,
	title = {A method of ranking interval numbers based on degrees for multiple attribute decision making},
	volume = {30},
	issn = {1064-1246},
	url = {https://content.iospress.com/articles/journal-of-intelligent-and-fuzzy-systems/ifs1747},
	doi = {10.3233/IFS-151747},
	abstract = {In order to deal with the difficulty of ranking interval numbers in the multiple attribute decision making process, interval numbers are expressed in the Rectangular Coordinate System. On the basis of this, two-dimensional relations of interval numbe},
	language = {en},
	number = {1},
	urldate = {2023-06-22},
	journal = {Journal of Intelligent \& Fuzzy Systems},
	author = {Ye, Yicheng and Yao, Nan and Wang, Qiaozhi and Wang, Qihu},
	month = jan,
	year = {2016},
	note = {Publisher: IOS Press},
	pages = {211--221},
	file = {Full Text:C\:\\Users\\aravind\\Zotero\\storage\\6RZPG3AI\\Ye et al. - 2016 - A method of ranking interval numbers based on degr.pdf:application/pdf},
}

@article{liu2014ranking,
	title = {Ranking grey numbers based on dominance grey degrees},
	volume = {25},
	issn = {1004-4132},
	doi = {10.1109/JSEE.2014.00072},
	abstract = {With respect to the decision making problems where a lot of fuzzy and grey information always exists in the real-life decision making information system, it is difficult for such uncertainty methods as fuzzy mathematics, probability, and interval numbers to deal with. To this end, based on the thought and method of grey numbers, grey degrees and interval numbers, the concept of dominance grey degree is defined. And then a method of ranking interval grey numbers based on the dominance grey degree is proposed. After discussing the relevant properties, the paper finally uses an example to demonstrate the effectiveness and applicability of the model. The result shows that the proposed model can more accurately describe uncertainty decision making problems, and realize the total ordering process for multiple-attribute decision-making problems.},
	number = {4},
	journal = {Journal of Systems Engineering and Electronics},
	author = {Liu, Yong and Forrest, Jeffrey and Xie, Naiming},
	month = aug,
	year = {2014},
	note = {Conference Name: Journal of Systems Engineering and Electronics},
	keywords = {Decision making, dominance grey degree, Fuzzy logic, grey information, Information systems, interval grey number, Probability, Reflective binary codes, total ordering, Uncertainty},
	pages = {618--626},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\aravind\\Zotero\\storage\\6EEWUUZC\\6905956.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\aravind\\Zotero\\storage\\9GCEHJTH\\Liu et al. - 2014 - Ranking grey numbers based on dominance grey degre.pdf:application/pdf},
}

@article{zhang2016ranking,
	series = {Three-way {Decisions} and {Granular} {Computing}},
	title = {Ranking interval sets based on inclusion measures and applications to three-way decisions},
	volume = {91},
	issn = {0950-7051},
	url = {https://www.sciencedirect.com/science/article/pii/S0950705115002798},
	doi = {10.1016/j.knosys.2015.07.025},
	abstract = {Three-way decisions provide an approach to obtain a ternary classification of the universe as acceptance region, rejection region and uncertainty region respectively. Interval set theory is a new tool for representing partially known concepts, especially it corresponds to a three-way decision. This paper proposes a framework for comparing two interval sets by inclusion measures. Firstly, we review the basic notations, interpretation and operation of interval sets and classify the orders on interval sets into partial order, preorder and quasi-order. Secondly, we define inclusion measure which indicates the degree to which one interval set is less than another one and construct different inclusion measures to present the quantitative ranking of interval sets. Furthermore, we present similarity measures and distances of interval sets and investigate their relationship with inclusion measures. In addition, we propose the fuzziness measure and ambiguity measure to show the uncertainty embedded in an interval set. Lastly, we study the application of inclusion measures, similarity measures and uncertainty measures of interval sets by a special case of three-way decisions: rough set model and the results show that these measures are efficient to three-way decision processing.},
	language = {en},
	urldate = {2023-06-22},
	journal = {Knowledge-Based Systems},
	author = {Zhang, Hong-Ying and Yang, Shu-Yun and Ma, Jian-Min},
	month = jan,
	year = {2016},
	keywords = {Inclusion measure, Interval set, Similarity measure, Three-way decisions, Uncertainty measure},
	pages = {62--70},
	file = {ScienceDirect Snapshot:C\:\\Users\\aravind\\Zotero\\storage\\KHF8GLR5\\S0950705115002798.html:text/html},
}

@article{pavan2004new-1,
	series = {Papers presented at the 5th {COLLOQUIUM} {CHEMIOMETRICUM} {MEDITERRANEUM}},
	title = {New indices for analysing partial ranking diagrams},
	volume = {515},
	issn = {0003-2670},
	url = {https://www.sciencedirect.com/science/article/pii/S0003267003014879},
	doi = {10.1016/j.aca.2003.11.019},
	abstract = {Interest is growing in decision making strategies and several techniques are now available. The assessment of priorities is a typical premise before final decisions are taken. Total and partial order ranking (POR) strategies, which from a mathematical point of view are based on elementary methods of discrete mathematics, appear as an attractive and simple tool to asses priorities. Despite the well-known total ranking strategies, which are scalar methods combining the different criteria values into a global index which always ranks elements in an ordered sequence, the partial order ranking is a vectorial approach which recognises that not all the elements can be directly compared with all the others. In fact when many criteria are considered, contradictions in the ranking are bound to exist and the higher the number of criteria, the higher the probability that contradictions in the ranking occur. The Hasse diagram technique (HDT) is a very useful tool to perform partial order ranking. The results of the partial order ranking are visualised in a diagram, called Hasse diagram. Incomparable elements are located at the same geometrical height and as high as possible in the diagram, thus incomparable elements are arranged in levels. The quality of a ranking procedure has to be evaluated by a deep analysis and by several indices, i.e. scalar functions that describe features of an ordered set and allow comparison among different rankings. For this purpose, new indices for ranking analysis are proposed here, compared with the ones found in literature and tested on theoretical examples and on real data.},
	language = {en},
	number = {1},
	urldate = {2023-06-22},
	journal = {Analytica Chimica Acta},
	author = {Pavan, M and Todeschini, R},
	month = jul,
	year = {2004},
	keywords = {Hasse diagrams, Multicriteria decision making, Partial order ranking, Priority setting, Ranking indices},
	pages = {167--181},
	file = {ScienceDirect Full Text PDF:C\:\\Users\\aravind\\Zotero\\storage\\UYZVJYZC\\Pavan and Todeschini - 2004 - New indices for analysing partial ranking diagrams.pdf:application/pdf;ScienceDirect Snapshot:C\:\\Users\\aravind\\Zotero\\storage\\GUX6XWMS\\S0003267003014879.html:text/html},
}

@article{ailon2010aggregation,
	title = {Aggregation of {Partial} {Rankings}, p-{Ratings} and {Top}-m {Lists}},
	volume = {57},
	issn = {1432-0541},
	url = {https://doi.org/10.1007/s00453-008-9211-1},
	doi = {10.1007/s00453-008-9211-1},
	abstract = {We study the problem of aggregating partial rankings. This problem is motivated by applications such as meta-searching and information retrieval, search engine spam fighting, e-commerce, learning from experts, analysis of population preference sampling, committee decision making and more. We improve recent constant factor approximation algorithms for aggregation of full rankings and generalize them to partial rankings. Our algorithms improve constant factor approximation with respect to a family of metrics recently proposed in the context of comparing partial rankings. We pay special attention to two important types of partial rankings: the well-known top-m lists and the more general p-ratings which we define. We provide first evidence for hardness of aggregating them for constant m, p.},
	language = {en},
	number = {2},
	urldate = {2023-06-22},
	journal = {Algorithmica},
	author = {Ailon, Nir},
	month = jun,
	year = {2010},
	keywords = {Approximation algorithms, Rank aggregation, Ranking with ties},
	pages = {284--300},
	file = {Full Text PDF:C\:\\Users\\aravind\\Zotero\\storage\\8ZJYJ5T5\\Ailon - 2010 - Aggregation of Partial Rankings, p-Ratings and Top.pdf:application/pdf},
}

@article{fagin2006comparing,
	title = {Comparing {Partial} {Rankings}},
	volume = {20},
	issn = {0895-4801},
	url = {https://epubs.siam.org/doi/abs/10.1137/05063088X},
	doi = {10.1137/05063088X},
	abstract = {Motivated by several applications, we introduce various distance measures between "top k lists." Some of these distance measures are metrics, while others are not. For each of these latter distance measures, we show that they are "almost" a metric in the following two seemingly unrelated aspects:(i) they satisfy a relaxed version of the polygonal (hence, triangle) inequality, and(ii) there is a metric with positive constant multiples that bound our measure above and below.This is not a coincidence---we show that these two notions of almost being a metric are the same. Based on the second notion, we define two distance measures to be equivalent if they are bounded above and below by constant multiples of each other. We thereby identify a large and robust equivalence class of distance measures.Besides the applications to the task of identifying good notions of (dis)similarity between two top k lists, our results imply polynomial-time constant-factor approximation algorithms for the rank aggregation problem with respect to a large class of distance measures. (A correction for this article has been appended to the pdf file.)},
	number = {3},
	urldate = {2023-06-22},
	journal = {SIAM Journal on Discrete Mathematics},
	author = {Fagin, Ronald and Kumar, Ravi and Mahdian, Mohammad and Sivakumar, D. and Vee, Erik},
	month = jan,
	year = {2006},
	note = {Publisher: Society for Industrial and Applied Mathematics},
	pages = {628--648},
	file = {Full Text PDF:C\:\\Users\\aravind\\Zotero\\storage\\CQY66JW9\\Fagin et al. - 2006 - Comparing Partial Rankings.pdf:application/pdf},
}

@incollection{bruggemann2011structures,
	address = {New York, NY},
	title = {Structures of {Partial} {Orders}},
	isbn = {978-1-4419-8476-0 978-1-4419-8477-7},
	url = {http://link.springer.com/10.1007/978-1-4419-8477-7_5},
	language = {en},
	urldate = {2023-06-22},
	booktitle = {Ranking and {Prioritization} for {Multi}-indicator {Systems}},
	publisher = {Springer New York},
	author = {Brüggemann, Rainer and Patil, Ganapati P.},
	collaborator = {Brüggemann, Rainer and Patil, Ganapati P.},
	year = {2011},
	doi = {10.1007/978-1-4419-8477-7_5},
	pages = {57--74},
}

@incollection{bruggemann2011partial,
	address = {New York, NY},
	title = {Partial {Order} and {Hasse} {Diagrams}},
	isbn = {978-1-4419-8476-0 978-1-4419-8477-7},
	url = {http://link.springer.com/10.1007/978-1-4419-8477-7_2},
	language = {en},
	urldate = {2023-06-22},
	booktitle = {Ranking and {Prioritization} for {Multi}-indicator {Systems}},
	publisher = {Springer New York},
	author = {Brüggemann, Rainer and Patil, Ganapati P.},
	collaborator = {Brüggemann, Rainer and Patil, Ganapati P.},
	year = {2011},
	doi = {10.1007/978-1-4419-8477-7_2},
	pages = {13--23},
}

@book{bruggemann2011ranking,
	title = {Ranking and {Prioritization} for {Multi}-indicator {Systems}: {Introduction} to {Partial} {Order} {Applications}},
	isbn = {978-1-4419-8477-7},
	shorttitle = {Ranking and {Prioritization} for {Multi}-indicator {Systems}},
	abstract = {This book provides axioms of partial order and some basic material, for example consequences of “criss-crossing” of data profiles, the role of aggregations of the indicators and the powerful method of formal concept analysis. The interested reader will learn how to apply fuzzy methods in partial order analysis and what ‘antagonistic indicator’ means.},
	language = {en},
	publisher = {Springer Science \& Business Media},
	author = {Brüggemann, Rainer and Patil, Ganapati P.},
	month = jul,
	year = {2011},
	note = {Google-Books-ID: OpUPQeD3CtgC},
	keywords = {Mathematics / Probability \& Statistics / General, Medical / Biostatistics, Science / General},
}

@inproceedings{janicki2008ranking,
	address = {Berlin, Heidelberg},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Ranking with {Partial} {Orders} and {Pairwise} {Comparisons}},
	isbn = {978-3-540-79721-0},
	doi = {10.1007/978-3-540-79721-0_61},
	abstract = {A new approach to Pairwise Comparisons based Ranking is presented. An abstract model based on partial orders instead of numerical scales is introduced and analysed. The importance of the concept of indifference and the power of weak order extensions are discussed.},
	language = {en},
	booktitle = {Rough {Sets} and {Knowledge} {Technology}},
	publisher = {Springer},
	author = {Janicki, Ryszard},
	editor = {Wang, Guoyin and Li, Tianrui and Grzymala-Busse, Jerzy W. and Miao, Duoqian and Skowron, Andrzej and Yao, Yiyu},
	year = {2008},
	keywords = {Pairwise Comparison, Partial Order, Ranking Problem, Ranking Relation, Total Order},
	pages = {442--451},
	file = {Full Text PDF:C\:\\Users\\aravind\\Zotero\\storage\\DJHVM748\\Janicki - 2008 - Ranking with Partial Orders and Pairwise Compariso.pdf:application/pdf},
}

@inproceedings{ding2001spectral,
	address = {New York, NY, USA},
	series = {{KDD} '01},
	title = {A spectral method to separate disconnected and nearly-disconnected web graph components},
	isbn = {978-1-58113-391-2},
	url = {https://dl.acm.org/doi/10.1145/502512.502551},
	doi = {10.1145/502512.502551},
	abstract = {Separation of connected components from a graph with disconnected graph components mostly use breadth-first search (BFS) or depth-first search (DFS) graph algorithms. Here we propose a new algebraic method to separate disconnected and nearly-disconnected components. This method is based on spectral graph partitioning, following a key observation that disconnected components will show up, after properly sorted, as step-function like curve in the lowest eigenvectors of the Laplacian matrix of the graph. Following an perturbative analysis framework, we systematically analyzed the graph structures, first on the disconnected subgraph case, and second on the effects of adding edges sparsely connecting different subgraphs as a perturbation. Several new results are derived, providing insights to spectral methods and related clustering objective function. Examples are given illustrating the concepts and results our methods. Comparing to the standard graph algorithms, this method has the same O(‖E ‖ + ‖V‖log(‖V‖)) complexity, but is easier to implement (using readily available eigensolvers). Further more the method can easily identify articulation points and bridges on nearly-disconnected graphs. Segmentation of a real example of Web graph for query amazon is given. We found that each disconnected or nearly-disconnected components forms a cluster on a clear topic.},
	urldate = {2023-09-15},
	booktitle = {Proceedings of the seventh {ACM} {SIGKDD} international conference on {Knowledge} discovery and data mining},
	publisher = {Association for Computing Machinery},
	author = {Ding, Chris H. Q. and He, Xiaofeng and Zha, Hongyuan},
	month = aug,
	year = {2001},
	pages = {275--280},
	file = {Full Text PDF:C\:\\Users\\aravind\\Zotero\\storage\\R6V8FWR8\\Ding et al. - 2001 - A spectral method to separate disconnected and nea.pdf:application/pdf},
}
